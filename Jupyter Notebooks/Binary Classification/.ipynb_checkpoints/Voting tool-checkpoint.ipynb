{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer                   \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.backend import clear_session\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pvuN_WvF1to</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRLJscAlk1M</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VbiRNT_gWUQ</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5dVcn8NjbwY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5scez5dqtAc</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>TZ0j6kr4ZJ0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>8DiWzvE52ZY</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>OwqIy8Ikv-c</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>lPgZfhnCAdI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>dSu5sXmsur4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VideoID  Effectiveness\n",
       "0    pvuN_WvF1to            1.0\n",
       "1    eRLJscAlk1M            5.0\n",
       "2    VbiRNT_gWUQ            2.0\n",
       "3    5dVcn8NjbwY            NaN\n",
       "4    5scez5dqtAc            4.0\n",
       "..           ...            ...\n",
       "186  TZ0j6kr4ZJ0            3.0\n",
       "187  8DiWzvE52ZY            1.0\n",
       "188  OwqIy8Ikv-c            2.0\n",
       "189  lPgZfhnCAdI            1.0\n",
       "190  dSu5sXmsur4            3.0\n",
       "\n",
       "[191 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../NLP Preprocessing/02_Resources/Climate_change_links.xlsx')\n",
    "data = df[[\"VideoID\", \"Effectiveness\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pvuN_WvF1to</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRLJscAlk1M</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VbiRNT_gWUQ</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5scez5dqtAc</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JDcro7dPqpA</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>TZ0j6kr4ZJ0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>8DiWzvE52ZY</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>OwqIy8Ikv-c</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>lPgZfhnCAdI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>dSu5sXmsur4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VideoID  Effectiveness\n",
       "0    pvuN_WvF1to            1.0\n",
       "1    eRLJscAlk1M            5.0\n",
       "2    VbiRNT_gWUQ            2.0\n",
       "3    5scez5dqtAc            4.0\n",
       "4    JDcro7dPqpA            2.0\n",
       "..           ...            ...\n",
       "164  TZ0j6kr4ZJ0            3.0\n",
       "165  8DiWzvE52ZY            1.0\n",
       "166  OwqIy8Ikv-c            2.0\n",
       "167  lPgZfhnCAdI            1.0\n",
       "168  dSu5sXmsur4            3.0\n",
       "\n",
       "[169 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean data\n",
    "data = data.loc[(data[\"Effectiveness\"] == 1) | (data[\"Effectiveness\"] == 2) | (data[\"Effectiveness\"] == 3) | (data[\"Effectiveness\"] == 4) | (data[\"Effectiveness\"] == 5)]\n",
    "data = data.reset_index()\n",
    "del data[\"index\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pvuN_WvF1to</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eRLJscAlk1M</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VbiRNT_gWUQ</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5scez5dqtAc</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JDcro7dPqpA</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>JYZpxRy5Mfg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>xXMlFFY9uEI</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>8DiWzvE52ZY</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>OwqIy8Ikv-c</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>lPgZfhnCAdI</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VideoID Effectiveness\n",
       "0    pvuN_WvF1to           neg\n",
       "1    eRLJscAlk1M           pos\n",
       "2    VbiRNT_gWUQ           neg\n",
       "3    5scez5dqtAc           pos\n",
       "4    JDcro7dPqpA           neg\n",
       "..           ...           ...\n",
       "132  JYZpxRy5Mfg           pos\n",
       "133  xXMlFFY9uEI           pos\n",
       "134  8DiWzvE52ZY           neg\n",
       "135  OwqIy8Ikv-c           neg\n",
       "136  lPgZfhnCAdI           neg\n",
       "\n",
       "[137 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Custom encoder\n",
    "def custom_encoder(df):\n",
    "    df.replace(to_replace = 1.0, value = \"neg\", inplace=True)\n",
    "    df.replace(to_replace = 2.0, value = \"neg\", inplace=True)\n",
    "    df.replace(to_replace = 4.0, value = \"pos\", inplace=True)\n",
    "    df.replace(to_replace = 5.0, value = \"pos\", inplace=True)\n",
    "\n",
    "custom_encoder(df['Effectiveness'])\n",
    "\n",
    "data = df[[\"VideoID\", \"Effectiveness\"]]\n",
    "data = data[data[\"Effectiveness\"] != 3]\n",
    "data = data.loc[(data[\"Effectiveness\"] == 'pos') | (data[\"Effectiveness\"] == 'neg')]\n",
    "data = data.reset_index()\n",
    "del data[\"index\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jared\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visulaise \n",
    "plot = sns.countplot(data['Effectiveness'], order=['neg','pos'])\n",
    "plot.set_xlabel(\"Effectiveness\", fontsize = 12)\n",
    "plot.set_ylabel(\"Count\", fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get documnets (pre-processd comments)\n",
    "documents = []\n",
    "for i in range(len(data)):\n",
    "    VideoID = data[\"VideoID\"][i]\n",
    "    comment = pd.read_csv(\"../NLP Preprocessing/03_Processed_Comments/\"+VideoID+\"/\"+VideoID+\"_all_words.csv\")\n",
    "    documents.append(list(comment[\"0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create two new columns of the pre-processed data in list and string form\n",
    "data['cleaned'] = documents\n",
    "data['cleaned_string'] = [' '.join(map(str, l)) for l in data['cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot confusion matrix\n",
    "def plot_conf_matrix(conf_matrix):\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                conf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         conf_matrix.flatten()/np.sum(conf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "    sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "    \n",
    "# function to get results of algorithm\n",
    "def get_results(y_test, y_pred):\n",
    "    # print classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # plot confusion matrix\n",
    "    plot_conf_matrix(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 1))\n",
    "\n",
    "# split training and test set\n",
    "X = data.cleaned_string\n",
    "y = data.Effectiveness\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pickles/randFor_71.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23884/396220722.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# open pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrandFor_71_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pickles/randFor_71.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mrandFor_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandFor_71_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrandFor_71_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickles/randFor_71.pickle'"
     ]
    }
   ],
   "source": [
    "# create pipeline for algorithm\n",
    "randFor = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', RandomForestClassifier(n_estimators=1000, random_state=1, criterion='entropy', oob_score=True, verbose=1)),\n",
    "                   ])\n",
    "\n",
    "# open pickle file\n",
    "randFor_71_f = open('pickles/randFor_71.pickle', \"rb\")\n",
    "randFor_train = pickle.load(randFor_71_f)\n",
    "randFor_71_f.close()\n",
    "\n",
    "y_pred_RandFor = randFor_train.predict(X_test)\n",
    "\n",
    "# get results of algorithm\n",
    "get_results(y_test, y_pred_RandFor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline for algorithm\n",
    "SGD = Pipeline([('vect', vectorizer),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, tol=None, random_state=12)),\n",
    "               ])\n",
    "\n",
    "# open pickle file\n",
    "SGD_74_f = open('pickles/SGD_74.pickle', \"rb\")\n",
    "SGD_train = pickle.load(SGD_74_f)\n",
    "SGD_74_f.close()\n",
    "\n",
    "# get prediction from algorithm\n",
    "y_pred_SGD = SGD_train.predict(X_test)\n",
    "\n",
    "# get results of algorithm\n",
    "get_results(y_test, y_pred_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline for algorithm\n",
    "XGB = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', XGBClassifier()),\n",
    "               ])\n",
    "\n",
    "# open pickle file\n",
    "XGB_74_f = open('pickles/XGB_74.pickle', \"rb\")\n",
    "XGB_train = pickle.load(XGB_74_f)\n",
    "XGB_74_f.close()\n",
    "\n",
    "# get prediction from algorithm\n",
    "y_pred_XGB = XGB_train.predict(X_test)\n",
    "\n",
    "# get results of algorithm\n",
    "get_results(y_test, y_pred_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline for algorithm\n",
    "logreg = Pipeline([('vect', vectorizer),\n",
    "                   ('clf', LogisticRegression(n_jobs=1, C=100000.0, penalty='l1', solver='liblinear', random_state=12)),\n",
    "                  ])\n",
    "\n",
    "# open pickle file\n",
    "logreg_79_f = open('pickles/logreg_79.pickle', \"rb\")\n",
    "logreg_train = pickle.load(logreg_79_f)\n",
    "logreg_79_f.close()\n",
    "\n",
    "# get prediction from algorithm\n",
    "y_pred_logreg = logreg_train.predict(X_test)\n",
    "\n",
    "# get results of algorithm\n",
    "get_results(y_test, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create CNN model\n",
    "model = Sequential()\n",
    "embedding_dim = 100\n",
    "vocab_size = 410796\n",
    "maxlen = 100\n",
    "# input embedded layer\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "# convolutional layer with relu activation\n",
    "model.add(layers.Conv1D(128, 7, activation='relu'))\n",
    "# max pooling layer\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "# hidden layer with 10 neurons with a softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "# output layer with a sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',\n",
    "                                                                     tf.keras.metrics.TruePositives(),\n",
    "                                                                     tf.keras.metrics.TrueNegatives(),\n",
    "                                                                     tf.keras.metrics.FalsePositives(),\n",
    "                                                                     tf.keras.metrics.FalseNegatives(),   \n",
    "                                                                     tf.keras.metrics.Precision(class_id=None),\n",
    "                                                                     tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open y_train file\n",
    "y_train_f = open('Deep learning/pickles/y_train.pickle', \"rb\")\n",
    "y_train_CNN = pickle.load(y_train_f)\n",
    "y_train_f.close()\n",
    "\n",
    "# open y_test file\n",
    "y_test_f = open('Deep learning/pickles/y_test.pickle', \"rb\")\n",
    "y_test_CNN = pickle.load(y_test_f)\n",
    "y_test_f.close()\n",
    "\n",
    "# open X_train file\n",
    "X_train_f = open('Deep learning/pickles/X_train.pickle', \"rb\")\n",
    "X_train_CNN = pickle.load(X_train_f)\n",
    "X_train_f.close()\n",
    "\n",
    "# open X_test file\n",
    "X_test_f = open('Deep learning/pickles/X_test.pickle', \"rb\")\n",
    "X_test_CNN = pickle.load(X_test_f)\n",
    "X_test_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get saved model\n",
    "model = keras.models.load_model(\"Deep learning/CNN_82\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, CNN_accuracy, tp, tn, fp, fn, precision, recall = model.evaluate(X_test_CNN, y_test_CNN, verbose=False)\n",
    "print(\"Testing Accuracy: \", CNN_accuracy*100)\n",
    "\n",
    "# Creating confusion matrix\n",
    "plot_conf_matrix(np.array([[tp, fp], \n",
    "                          [fn, tn]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate algorithms\n",
    "def evaluate(y_test, y_pred_dict):\n",
    "    table = pd.DataFrame({}, index=['Accuracy', 'Precision', 'Recall','F1-score']) \n",
    "    \n",
    "    for model in y_pred_dict:\n",
    "        report = classification_report(y_test, y_pred_dict[model], digits=2, output_dict=True)\n",
    "        \n",
    "        cols = [report['accuracy'],(report['neg']['precision']+report['pos']['precision'])/2,(report['neg']['recall']+report['pos']['recall'])/2,(report['neg']['f1-score']+report['pos']['f1-score'])/2]\n",
    "        table[model] = cols\n",
    "    \n",
    "    # add CNN results\n",
    "    table.insert(table.shape[1],\"CNN\", [0.8235,0.8888,0.6153,0.7272], True)\n",
    "    \n",
    "    # convert to percentage\n",
    "    table = table*100\n",
    "    \n",
    "    # Add 'Best Score' column\n",
    "    table['Best Score'] = table.idxmax(axis=1)\n",
    "    \n",
    "    return table.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of algorithms and their respective predictions\n",
    "y_pred_dict = {\n",
    "               \"Random Forest\": y_pred_RandFor,\n",
    "               \"SGD\": y_pred_SGD, \n",
    "               \"XGBoost\": y_pred_XGB,\n",
    "               \"Logistic Regression\": y_pred_logreg,\n",
    "              }\n",
    "\n",
    "table = evaluate(y_test, y_pred_dict)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot\n",
    "# get only accuracy and F1-score\n",
    "accuracy_f1 = table.iloc[[0,3]]\n",
    "accuracy_f1.drop(accuracy_f1.columns[len(accuracy_f1.columns)-1], axis=1, inplace=True)\n",
    "# get all algoritms with accuracy >= 70\n",
    "accuracy_f1 = accuracy_f1.loc[:, accuracy_f1.gt(70).any()]\n",
    "\n",
    "# convert numpy array into list\n",
    "accuracy_values = [item for sublist in accuracy_f1.iloc[[0]].values.tolist() for item in sublist]\n",
    "f1_values = [item for sublist in accuracy_f1.iloc[[1]].values.tolist() for item in sublist]\n",
    "\n",
    "plotdata = pd.DataFrame({\n",
    "    \"Accuracy\" : accuracy_values,\n",
    "    \"F1-Score\" : f1_values\n",
    "    }, \n",
    "    index = list(accuracy_f1)\n",
    ")\n",
    "plotdata.plot(kind=\"bar\")   \n",
    "\n",
    "plt.title(\"Best Performing Algorithms\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.ylabel(\"Accuracy and F1-Score (%)\")\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.ylim([0,100])\n",
    "plt.legend(loc='best')\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]    \n",
    "#plt.savefig('Best Performing Algorithms.png', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get YouTube ID\n",
    "def getID(url):\n",
    "    from pytube import extract\n",
    "    print(\"Getting YouTube ID...\")\n",
    "    return extract.video_id(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download comments\n",
    "def downloadComments(videoID):\n",
    "    import os\n",
    "    print(\"Downloading Comments...\")\n",
    "    os.system(\"youtube-comment-downloader --youtubeid=\" + videoID + \" --output test/Comments/\" + videoID + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import nltk.sentiment.util\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to clean comments\n",
    "def clean_text(text):\n",
    "    \n",
    "    # remove symbols and Emojis\n",
    "    text = text.lower()\n",
    "    text = re.sub('@', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r\"[^a-zA-Z ]+\", \"\", text)\n",
    "    \n",
    "    # tokenize the data\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # lemmatize\n",
    "    text = [lemmatizer.lemmatize(t) for t in text]\n",
    "    text = [lemmatizer.lemmatize(t, 'v') for t in text]\n",
    "    \n",
    "    # mark Negation\n",
    "    tokens_neg_marked = nltk.sentiment.util.mark_negation(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = [t for t in tokens_neg_marked\n",
    "             if t.replace(\"_NEG\", \"\").isalnum() and\n",
    "             t.replace(\"_NEG\", \"\") not in sw]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean comments\n",
    "print(\"Cleaning Comments...\")\n",
    "df = pd.read_json('test/Comments/'+ videoID + '.json', lines=True)\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "all_words = []        \n",
    "for i in range(len(df)):\n",
    "    all_words = all_words + df['text'][i]\n",
    "\n",
    "df.to_json('test/Processed Comments/'+ videoID +'.json', orient=\"records\", lines=True)\n",
    "df_csv = pd.DataFrame(all_words)\n",
    "df_csv.to_csv('test/Processed Comments/' + videoID + '_all_words.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame([[videoID]], columns=['VideoID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get documnets (pre-processd comments)\n",
    "test_documents = []\n",
    "for i in range(len(test)):\n",
    "    VideoID = test[\"VideoID\"][i]\n",
    "    comment = pd.read_csv(\"test/Processed Comments/\"+VideoID+\"_all_words.csv\")\n",
    "    test_documents.append(list(comment[\"0\"]))\n",
    "    \n",
    "## create two new columns of the pre-processed data in list and string form\n",
    "test['cleaned'] = test_documents\n",
    "test['cleaned_string'] = [' '.join(map(str, l)) for l in test['cleaned']]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test point\n",
    "test_point = test.cleaned_string\n",
    "test_sentence = test['cleaned_string'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open sentences_train file\n",
    "sentences_train_f = open('Deep learning/pickles/sentences_train.pickle', \"rb\")\n",
    "sentences_train = pickle.load(sentences_train_f)\n",
    "sentences_train_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceTrain():\n",
    "    # open sentences_train file\n",
    "    sentences_train_f = open('Deep learning/pickles/sentences_train.pickle', \"rb\")\n",
    "    sentences_train = pickle.load(sentences_train_f)\n",
    "    sentences_train_f.close()\n",
    "    return sentences_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "print(\"Tokenizing the data...\")\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test = pad_sequences(tokenizer.texts_to_sequences(test_sentence), padding='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"pickles/tokenizer.pickle\",\"wb\")\n",
    "pickle.dump(tokenizer, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote(test_point, _test):\n",
    "    print(\"Voting on video effectivess...\\n\")\n",
    "    pos_weighting = []\n",
    "    result = ''\n",
    "    confidence = 0\n",
    "    algos_score = 0\n",
    "    \n",
    "    algorithms = [\n",
    "             {'name': 'Random Forest', 'accuracy': accuracy_score(y_test, y_pred_RandFor)*100, 'trained': randFor_train},\n",
    "             {'name': 'SGD', 'accuracy': accuracy_score(y_test, y_pred_SGD)*100, 'trained': SGD_train},\n",
    "             {'name': 'XGBoost', 'accuracy':  accuracy_score(y_test, y_pred_SGD)*100, 'trained': XGB_train},\n",
    "             {'name': 'Logistic Regression', 'accuracy': accuracy_score(y_test, y_pred_logreg)*100, 'trained': logreg_train},\n",
    "             {'name': 'CNN', 'accuracy': CNN_accuracy*100, 'trained': model}\n",
    "        ]\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        weight = algo['accuracy']\n",
    "        algos_score += weight\n",
    "        if algo['name'] == \"CNN\":\n",
    "            pred = algo['trained'].predict(_test)\n",
    "            if pred[0][0] > 0.5:\n",
    "                pos_weighting.append(weight)\n",
    "            print(\"CNN voted for: effective\" if pred[0][0]>0.5 else \"CNN voted for: ineffective\")\n",
    "        else:\n",
    "            pred = algo['trained'].predict(test_point)\n",
    "            if pred == 'pos':\n",
    "                pos_weighting.append(weight)\n",
    "            print(algo['name'] + \" voted for: effective\" if pred=='pos' else algo['name'] + \" voted for: ineffective\")\n",
    "\n",
    "    pos_result = sum(pos_weighting)/algos_score\n",
    "    if pos_result < 0.5:\n",
    "        result = 'ineffective'\n",
    "        confidence = 1-pos_result\n",
    "    else:\n",
    "        result = 'effective'\n",
    "        confidence = pos_result\n",
    "        \n",
    "    print(\"\\nThis video is \\033[1m\" + result + \"\\033[0m with a confidence of \\033[1m\" + str(round(confidence*100,2)) + \"% \\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote(test_point,_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeEffectiveness(url):\n",
    "    # 1. Get YouTube ID\n",
    "    videoID = getID(url)\n",
    "    \n",
    "    # 2. Download comments\n",
    "    downloadComments(videoID)\n",
    "    \n",
    "    # 3. Clean comments\n",
    "    print(\"Cleaning Comments...\")\n",
    "    df = pd.read_json('test/Comments/'+ videoID + '.json', lines=True)\n",
    "    df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "    all_words = []        \n",
    "    for i in range(len(df)):\n",
    "        all_words = all_words + df['text'][i]\n",
    "\n",
    "    df_csv = pd.DataFrame(all_words)\n",
    "    df_csv.to_csv('test/Processed Comments/' + videoID + '_all_words.csv', index=False)\n",
    "    \n",
    "    # 4. Create test dataframe\n",
    "    test = pd.DataFrame([[videoID]], columns=['VideoID'])\n",
    "    \n",
    "    # 5. Get documnets (pre-processd comments)\n",
    "    test_documents = []\n",
    "    comment = pd.read_csv(\"test/Processed Comments/\" + videoID + \"_all_words.csv\")\n",
    "    test_documents.append(list(comment[\"0\"]))\n",
    "    test['cleaned'] = test_documents\n",
    "    test['cleaned_string'] = [' '.join(map(str, l)) for l in test['cleaned']]\n",
    "    \n",
    "    # 6. Get ML test point\n",
    "    test_point = test.cleaned_string\n",
    "    test_sentence = test['cleaned_string'].values\n",
    "    \n",
    "    # 7. Get trained sentences\n",
    "    sentences_train = getSentenceTrain()\n",
    "    \n",
    "    # 8. Tokenize the data\n",
    "    print(\"Tokenizing the data...\")\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    \n",
    "    # 9. Get DL test point\n",
    "    _test = pad_sequences(tokenizer.texts_to_sequences(test_sentence), padding='post', maxlen=100)\n",
    "    \n",
    "    # 10. Vote on video effectiveness\n",
    "    vote(test_point,_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizeEffectiveness(\"https://www.youtube.com/watch?v=-AvTCYYs9t8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
